<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"><html><head>
	<meta http-equiv="Content-Type" content="text/html; charset=windows-1250"/>
	<meta name="Author" content="Ondra ika, ondra at dynawest.cz; Design by Petr Záveskı, petr.zavesky@seznam.cz"/>
	<meta name="Keywords" content=""/>
	<meta name="Description" content=""/>
	<script type="text/javascript" charset="windows-1250" src="fce.js"></script>
<style type="text/css">
	/* * { margin: 0; padding: 0; } */
	body { background-color: #ffffff; font-family: "Arial CE", "Helvetica CE", "Verdana CE", Arial, Helvetica, Verdana, sans-serif; }
</style>
<script type="text/javascript">
</script>
<title> Pong AI dokumentace </title></head>
<body>
	<!--
	<h1>
		<p>Dokumentace k Pong AI - umìlá inteligence pro hru pong.</p>
		<p>projekt na SIN</p>
	</h1>
	-->
	
	<h1>Dokumentace k Pong AI - umìlá inteligence pro hru pong</h1>
	
	<h2>Struktura</h2>
	<p>Pro ovládání aktivních prvkù ve høe Pong slouí agent s umìlou inteligencí.
	Inteligence je implementována pomocí zpìtnovazebního uèení (RE)
	s pouitím neuronové sítì pro ohodnocovací funkci <strong>Q</strong>.
	</p>
	
	<h2>Neuronová sí - funkce</h2>
	<p>Naše sí funguje na bìnıch principech. Jedná se o orientovanı graf, jeho uzly jsou 
	neurony a jsou propojeny ohodnocenımi hranami, které znamenají váhu.
	Nìkteré neurony jsou tzv. vstupní, resp. vıstupní; jejich seskupení tvoøí vstupní, resp. vıstupní vrstvu,
	do které se vkládá vstup pro vıpoèet, resp. ze které se odeèítá vısledek.
	</p><p>
	Vıpoèet probíhá klasicky - pro kadı neuron jsou vynásobeny jeho vstupy  vahou na daném vstupu,
	souèet tìchto vısledkù je potom vstupem pro funkci neuronu, obvykle
	<a href="http://en.wikipedia.org/wiki/Sigmoid_function">sigmoidu</a> (funkce je ale volitelná).
	Vıstup z této funkce je pak vıstupem neuronu.	
	</p><p>Tento vıpoèet probíhá v poøadí takovém, aby všechny vstupy poèítaného neuronu byly ji vypoètené.
	Pro optimalizaci rychlosti vıpoètu se nepoèítá s cyklickım zapojením neuronù.
	</p><p>Zapojení neuronù mùe bıt libovolné pøi zachování podmínky neexistence cyklù.
	Sí tedy nemusí bıt striktnì vrstvená. Tato vıhoda nevedla ke sníení vıkonu oproti
	implementaci poèítající pouze s vrstvenou sítí.
	</p>
	
	<h2>Neuronová sí - implementace</h2>
	<p>Pro potøeby implementace RE byla nejprve implementována neuronová sí s backpropagation.
	Jedná se o obecnou neuronovou sí specializovanou pomocí dìdìní.
	</p>
	<ul> <li><strong>{@link pongai.cNeuron cNeuron}</strong> - základní stavební prvek sítì.
	
	</li><li><strong>{@link pongai.cSynapse cSynapse}</strong> - spojení mezi neurony.
	
  </li><li><strong>{@link pongai.cNeuralNet cNeuralNet}</strong> - obecná neuronová sí s libovolnım poètem a zapojením neuronù.
		Metoda <code>Compute</code> poèítá s neexistencí cyklù v síti.
			
  </li><li><strong>{@link pongai.cNeuralNetIOAdapter cNeuralNetIOAdapter}</strong> - rozhraní neuronové sítì pro vkládání vstupù
		(<code>SetInputValues(double[] adValues)</code>) a pro odeèet vısledkù
		(<code>double[] GetOutputValues()</code>).
		
  </li><li><strong>{@link pongai.cNeuralNetPerceptron cNeuralNetPerceptron}</strong> - specializace <code>cNeuralNet</code>.
		Pøetíen je jen konstruktor, kterı podle pole poètu neuronù ve vrstvách
		vytvoøí pøíslušnı poèet neuronù, propojí je, a neurony
		z první, resp. poslední vrstvy urèí jako vstupní, resp. vıstupní.
		
  </li><li><strong>{@link pongai.cNeuralNetTeacher cNeuralNetTeacher}</strong> - uèitel neuronové sítì; uèí metodou backpropagation.
		Pracuje s objektem <code>cNeuralNetIOAdapter</code> a jeho odkazem na objekt <code>cNeuralNet</code>.
		Obsahuje trénovací sadu vzorù a poadovanıch vısledkù. Na základì odchylky skuteèného vısledku sítì
		od poadovaného (pøímo) upravuje jednotlivé váhy na synapsích mezi neurony sítì.
		
  </li><li><strong>cRound</strong> - tøída s rùznımi statickımi pomocnımi funkcemi.
  </li></ul>
	
	<!-- Agent - funkce -->
	<h2>Agent - funkce</h2>
	<p>Hra Pong je popsána v pøíslušné èásti dokumentace. Tam jsou popsány pojmy jako brána, míèek, gól atp.</p>
	<p>Jeliko jednoho agenta v modelu svìta pøedstavuje jedna pohyblivá hrací ploška,
	budou se v následujících odstavcích tyto dva pojmy pro struènost mírnì prolínat.</p>
	
	<p>Agent má "nastarosti" jednu plošku, její pomocí sleduje tyto cíle:</p>
		<ul style="list-style-type: decimal;"> <li>zabránit tomu, aby míèek pronikl a do jeho brány (nedostat gól)
    </li><li>dostat míèek do soupeøovy brány (dát mu gól)
    </li></ul>
	
	<p>Agent dostává od modelu prostøedí v pravidelnıch intervalech tyto informace:</p>
		<ul> <li>Pozice pohyblivıch prvkù (X) - tedy pozice jeho samotného a soupeøova pozice.
    </li><li>Pozice míèku (X,Y) - normalizováno na interval &lt;0,1&gt;
    </li><li>Smìr a rychlost pohybu míèku jako dvouslokovı vektor.
    </li></ul>
		
	<p>Pøi obdrení tìchto informací s nimi agent naloí tak, e podle nich upraví svùj vnitøní stav.
		Tyto informace jsou prakticky pøímo "namapovány" na promìnné v jeho stavu.</p>
	<p>Na ádost od agent podle svého souèasného stavu "vymyslí", jaká akce by mìla bıt provedena s jeho ploškou,
		a tuto informaci vrátí. Agent je lehce nedeterministickı v závislosti na konstatì pravdìpodobnosti vıbìru
		akce, která podle souèasnıch agentovıch zkušeností povede ke kladnému ohodnocení. Mùe se tedy stát,
		e vrátí jiné instrukce ve dvou následujících ádostech, ani by mezi nimi byl upraven stav.
		Nedeterministiènost je zavedena za úèelem "vyskoèení" z pøípadného lokálního minima.</p>
	
	<p>Agent se uèí na principu zpìtnovazebního uèení (reinforcement learning, RE).
	To znamená, e informaci o správnosti èi úspìšnosti svého konání nedostává okamitì po provedení akce,
	ale øídce, nepravidelnì, obvykle na základì nìjaké události, a v hodnocení není pøíliš informací,
	obvykle jen kladné èi záporné èíslo rùznıch velikostí urèující prospìšnost a závanost dané události.</p>
	
	<p>Pùvodní plán byl takovı, e náš agent ze hry Pong jako ohodnocení èinnosti bude dostávat body:</p>
		<ul> <li>+100 bodù, pokud soupeø dostane gól. 
    </li><li>-100 bodù, pokud agent sám dostane gól do vlastní brány.
    </li></ul>
	<p>Tento pøístup se však v prùbìhu vıvoje ukázal bıt jako ne zcela vhodnı, viz nadøazená èást dokumentace.</p>
		
		
	<!-- Agent - implementace -->
	<h2>Agent - implementace</h2>
	<p>Tøída Agenta byla vytvoøena v rámci balíèku objects a byla promíchána funkènost agenta a plošky.
	Tato èást pojednává pouze o èástech èásti tıkajících se implementace agenta jako pøedstavitele umìlé inteligence.
	</p>
	
	<p>Metoda uèení vzniklá kombinací metody Monte Carlo a ukládání "eligibility", podle návrhu popsaného v {@link overview pøehledu }
	byla implementována ve tøídì {@link objects.Agent Agent }. Tato tøída je sice v balíèku <code>objects</code>,
	patøí ale spíše do <code>pongai</code>, proto je ideovì popsána zde.</p>
	
	<h3>Pozorování a ovlivòování prostøedí</h3>
	
	<p>Agent pozoruje i ovlivòuje prostøedí pøedevším prostøednictvím metody
		{@link objects.Agent#doAction doAction() } V ní se dìje toto:</p>
	<ul> <li>Odeètou se hodnoty z modelu prostøedí.
	</li><li>Hodnoty se znormalizují: Rychlost se pøevede z rozsahu (-oo, +oo) na rozsah (0,1).
  </li><li>Tyto hodnoty se vloí na vstup agentovy neuronové sítì.
  </li><li>Provede se vıpoèet neuronové sítì.
  </li><li>Odeètou se vıstupní hodnoty sítì, které pøedstavují pravdìpodobnost vıberu jednotlivıch akcí.
  </li><li>Zjistí se, která z akcí má nejvyšší pravdìpodobnost.
  </li><li>Podle metody epsilon-greedy je vybrána akce, která se má provést, a ta je provedena.
  </li></ul>
	<p>A sem je to postup bìnı pro Q-learning s aplikací neurnové sítì. Nakonec provedeme jeden krok navíc:</p>
	<ul> <li>Uloíme si informace o této iteraci - jaké hodnoty jsme poskytli neuronové sítí, a jakou akci jsme vybrali.
		Tyto informace se ukládají do fronty FIFO s pevnou délkou N. Jsou v ní tedy informace o maximálnì N posledních iteracích.
  </li></ul>
	
	
	<div><img src="doc-files/rozlozeni_vah.gif" alt="" width="707" height="235" /></div>
		
	<h3>Událost "trefa míèku"</h3>
	<p>Kdy se míèek dotkne plošky, kterou má agent nastarosti, je zavolána jeho metoda {@link objects.Agent#hitReceived},
	ve které se zkopíruje aktuální obsah FIFO fronty, tedy informace o nìkolika posledních iteracích.
	Tato sada se uloí do skladu (interní tøída <code>cHitsStore</code>), kde se ukládají takovéto sady pro celou 
	aktuální epizodu (za posledních nìkolik set a tisíc iterací, starší se zahazují).
	</p>
	<p>Této události na obrázku odpovídají vrchol <strong>èerné</strong> køivky.</p>
	
	
	<h3>Zpìtná vazba</h3>
	
	<p>Hodnocení chování agent dostává prostøednictvím metod {@link objects.Agent#goalMaked } a {@link objects.Agent#goalReceived}.
	Tyto metody jsou volány, kdy agent dá, resp. dostane gól.
	V nich se zavolá metoda (@link #AdaptNeuralNet} s hodnotou odmìny +100, resp. -100.
	</p>
	<p>Této události na obrázku odpovídají vrchol <strong style="color: blue;">modré</strong> køivky.</p>
	
	
	<h3>Uèení</h3>
	
	<p>Samotné uèení probíhá v metodì (@link #AdaptNeuralNet}. Probíhá takto:</p>
	<ul> <li>V objektu <code>cHitsStore</code> máme informace o dìní pøed dùleitımi událostmi:
		Nìkolik iterací pøed kadım zásahem plošky míèkem v nedávné dobì bìhem této epizody.
		(Vıznam pojmù "nìkolik" a "nedávné" závisí na konstantì.)
  </li><li>Všem zásahùm je urèena "váha" v rozmezí <0,1> podle jejich stáøí - èím starší, tím ménì.
  </li><li>U všech zásahù pøiøadíme jednotlivım iteracím váhu podobnım zpùsobem.
  </li><li>Procházíme iterace od nejmladší po nejstarší:
  </li><li>Pro kadou iteraci nastavíme neuronové síti na vstup hodnoty uloené u této iterace
		a provedeme vıpoèet sítì (za úèelem správného nastavení vnitøních hodnot pro pouití back-propagation).
  </li><li>Vytvoøíme kopii uloenıch hodnot. U vıstupu, kterı pøedstavuje akci, která byla v dané iteraci provedena,
		nastavíme rozdíl podle váhy dané iterace a podle odmìny (rozdíl tedy mùe bıt kladnı i zápornı).
	</li><li>Tyto vstupní hodnoty a vıstupní hodnoty s jednou z nich upravenou poskytneme uèiteli sítì
		(objekt {@link pongai.cNeuralNetTeacher}) a provedeme distribuci chyby metodou back-propagation.
  </li></ul>
	<p>Tím by mìlo bıt zajištìno, e:</p>
	<ul> <li>pravdìpodobnost vıbìru akce vedoucí v urèitém stavu (a jim blízkıch) k obdrení gólu se sníí,
  </li><li>pravdìpodobnost vıbìru akce vedoucí v urèitém stavu (a jim blízkıch) ke gólu soupeøi se zvıší.
  </li></ul>
	<p>A jestli ne, a mì zašlápne obøí pštros.</p>
	

	<h3>Response welcome</h3>
	<p>Pokud je uvedenı postup principiálnì špatnı, dejte prosím vìdìt autorovi,
		kterı se velmi rád dozví, proè, a jakı postup by byl lepší. Díky.</p>
		

	
	
</body></html>